Part Two:
	PROCESS MANAGEMENT
	A process can be thought of as a program in execution. It needs resources, such as CPU time, memory, files,
and I/O devices, to accomplish its task. These resources are allocated to the process during its creation or while it
is being executed.

Chapter 3:
	PROCESSES
	A process is the unit of work in a modern time-sharing system. A system therefore consists of a collection of
processes: operating-system processes executing system code and user processes executing user code.
3.1 PROCESS CONCEPT
	A batch system executes jobs, whereas a time-shared system has user programs, or tasks. All these activities
are similar, so we call all of them processes.
	There is an emphasize that a program by itself is not a process, but a passive entity, such as a file
containing a list of instructions stored on disk (often called an executable file). A process is an active entity,
with a program counter specifying the next instruction to execute and a set of associated resources.

3.1.2 Process State
	As a process executes, it changes state.
1. New - being created.
2. Running - being executed.
3. Waiting - waiting for some event to occur.
4. Ready - waiting to be assigned to a processor.
5. Terminated - has finished executing.

3.1.4 Threads
	A process is a program that performs a single thread of execution.
3.2 PROCESS SCHEDULING
	The process scheduler selects an available process for execution on the CPU. The rest of the process will have
to wait until more processors have become available.
	When an interrupt occurs, the system needs to save the current context of the process running on the CPU so
that it can resotre that context when its processing is done. This task is know as a context switch. Time spent doing
context switching is pure overhead.

pg 118 (Example of using for and exec commands)

A process is independent if it cannot affect or be affected by the other processes executing in the system.
Cooperating processes require an interprocess communication(IPC).

3.4.2.3 Buffering
	Messages exchanged by communicating processes reside in a temporary queue. There are three ways of
implementation:
1. Zero Capacity
2. Bounded Capacity
3. Unbounded Capacity

3.5 EXAMPLES OF IPC SYSTEMS
	3.5.1 An Example: POSIX Shared Memory
	This part of the book has a great example of how to implement a shared-memory object using the shm_open()
system call.

/*This section of the book is filled with great examples of how to implement interprocess communications*/

3.6 COMMUNICATION IN CLIENT-SERVER SYSTEMS
	A socket is defined as an endpoint for communication. A pait of processes communicating over a network employ
a pair of sockets - one for each process. Communication using sockets is considered a low-level form of communication between distributed processes.
****Those interested in socket programming in C or C++, consult bibliographical notes at the end of this chapter.

3.6.3 Pipes
	A pipe acts as a conduit allowing two processes to communicate. When implementing, four things must be
considered:
1. Bidirectional, or unidirectional communication?
2. Bidirectional, travel one way (half duplex), or both ways (duplex) at the same time.
3. Parent child relationship exist?
4. Communication over network, or only locally?


3.7 SUMMARY
	Communication in client-server systems may use:
1. sockets
2. remote procedure calls (RPCs)
3. pipes

Chapter 4:
	THREADS
4.1 Overvies
	A thread is a basic unit of CPU utilization; it comprises a thread ID, a program counter, a register set, and a stack.

4.2 MULTICORE PROGRAMMING
	Multithreaded programming provides a mechanism for more efficient use of these multiply computing cores and improved concurrency.
	Data parallelism focuses on distributing subsets of the same data across multiple computing cores and performing the same operation on each core. Task parallelism involves distributing not data but tasks (threads) across multiple computing cores.\

4.3 MULTITHREADING MODELS
	1. Many-to-One Model
	2. One-to-One Model
	3. Many-to-Many Model

4.4 THREAD LIBRARIES
	Two general strategies for creating multople threads: asynchronous threading, parent creates a child thread and the parent resumes its execution, so that both parent and child execute concurrently. Synchronous threading, parent creates multiple child threadss and must wait for all children to finish executing. Pg 196 has examples of implementation of ptreads.
	
4.5 IMPLICIT THREADING
	The general idesa behind a thread pool is to create a number of threads at process startup and place them into a pool, where they sit and wait for work.
	Thread pools offer these benefits:
	1. Servicing a request on an existing thread is faster the waiting to create a thread.
	2. Thread pools limit the amount of thread that exisit at any point.
	3. Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running 	    the task.
	
4.6 THREADING ISSUES
4.6.2 Signal Hanlding
	A signal is used in UNIC systems to notify a process that a particular event has occured. All signals follow the same pattern:
		1. Signals are generated by the occurrence of a particular event.
		2. The signal is delivered to a process.
		3. Once delivered, the signal must be handled.
	A signal must be handled by one of two possible handlers:
		1. A default signal handler
		2. a user-defined signal handler

4.7 OPERATING-SYSTEMS EXAMPLES
	pg 211
	
CHAPTER 5: PROCESS SYNCHRONIZATION
	A cooperating process is one that can be affected or affect other processes that are executing.
	
5.2 The Critical-Section Problem
	Two general approaches to handling critical sections in operating systems: preemptive kernels and nonpreemptive kernels.
Preemptive Kernel:
	Allows a process to be preempted while it is running in kernel mode.
Nonpreemptive Kernel:
	Does not allow a process to be preempted while it is running in kernel mode.
5.3 Peterson's Solution
	We prove that the solution is correct.
	1. Mutual exclusion is preserved.
	2. The progress requirement is satisfied.
	3. The bounded-waiting requirement is met.

5.4 SYNCHRONIZATION HARDWARE
	Locking - protecting critical regions through the use of locks.

5.5 MUTEX LOCKS
	We use the mutex lock to protect critical regions and thus prevent race conditions. This type of mutex lock is also called a spinlock because the process "Spins" while waiting for the lock to become available.

5.6 SEMAPHORES
	A semaphore is an integer variable that, apart from initialization, is accessed only through two standard atomic operatoins: wait() and signal().
5.6.1 Semaphore Usage
	There are two different types of semaphores:
		1. Counting Semaphore - unrestriced domanin, can be used control access to limited resources.
		2. Binary Semaphore - can range between 0 and 1 (Similar to a mutex lock).
		
5.6.3 Deadlocks and Starvation
	When two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes, this sate is call a deadlock. Another problem related to deadlocks is indefinite blocking or starvation.

5.8 Monitors (More research)
	Abstract data type encapsulates data with a set of functions to operate on that data. A monitor type is an ADT that includes a  a set of programmer defined operations that are provided with mutual exclusion within the monitor. The monitor type also declares the variables whose values degine the state of an instance of that type, along with the bodies of functions that operate on those variables.
pg 250 gives an example of a monitor.

pg 260 gives an example on how to use unnamed semaphores.

Look it OpenMP

5.11 Deadlocks
	A process must request a resource before using it and must release the resource after using it.
		1. Request
		2. Use
		3. Release
	A deadlock situation can arise if the following four conditions hold simultaneously in a system:
		1. Mutual Exclusion
		2. Hold and Wait
		3. No preemption
		4. Circular Wait
	System Resource-Allocation graph:
	A directed edge P(i) -> R(j) is called a request edge.
	A directed edge R(j) -> P(i) is called an assignment edge.
	
	In summary, if a resource allocation graph does not have a cycle, then the system is not in a deadlocked state.
5.11.3 Methods for Handling Deadlocks
	1. We can use a protocol to prevent deadlocks, ensuring that the system will never enter a deadlocked state.
	2. We can allow the system to enter a deadlocked state, detect it, and recover.
	3. We can ignore the problem altogether and pretend that deadlocks never occur in the system.
	
CHAPTER 6: CPU SCHEDULING
